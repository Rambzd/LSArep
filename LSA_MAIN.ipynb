{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAMPLE TEXT\n",
    "postDoc = pd.read_csv(r\"https://raw.githubusercontent.com/Rambzd/Latent_Semantic_Analysis/main/body.csv\")\n",
    "df = pd.DataFrame(postDoc)\n",
    "#STOPWORDS\n",
    "stop_words = pd.read_csv(r\"https://raw.githubusercontent.com/Rambzd/Latent_Semantic_Analysis/main/stopwords.txt\") \n",
    "\n",
    "#TOKENIZE, LOWER CASE\n",
    "tokenized_documents = [word_tokenize(sentence.lower()) for sentence in df['sentence']]\n",
    "filtered_documents = []\n",
    "for sentence in tokenized_documents:\n",
    "    allowed_words = []\n",
    "    for word in sentence:\n",
    "        if word not in stop_words:\n",
    "            allowed_words.append(word)\n",
    "    filtered_documents.append(allowed_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     above  adore  again  agreeing  ah  aidenn  air  all  an  ancient  ...  \\\n",
      "0        0      0      0         0   0       0    0    0   0        0  ...   \n",
      "1        0      0      0         0   0       0    0    0   0        0  ...   \n",
      "2        0      0      0         0   0       0    0    1   0        0  ...   \n",
      "3        0      0      0         0   0       0    0    0   0        0  ...   \n",
      "4        0      0      0         0   0       0    0    0   0        0  ...   \n",
      "..     ...    ...    ...       ...  ..     ...  ...  ...  ..      ...  ...   \n",
      "158      1      0      0         0   0       0    0    0   0        0  ...   \n",
      "159      0      0      0         0   0       0    0    1   0        0  ...   \n",
      "160      0      0      0         0   0       0    0    0   0        0  ...   \n",
      "161      0      0      0         0   0       0    0    0   0        0  ...   \n",
      "162      0      0      0         0   0       0    0    0   0        0  ...   \n",
      "\n",
      "     wondering  word  wore  world  wretch  wrought  yet  yore  you  your  \n",
      "0            0     0     0      0       0        0    0     0    0     0  \n",
      "1            0     0     0      0       0        0    0     0    0     0  \n",
      "2            0     0     0      0       0        0    0     0    0     0  \n",
      "3            0     0     0      0       0        0    0     0    0     0  \n",
      "4            0     0     0      0       0        0    0     0    0     0  \n",
      "..         ...   ...   ...    ...     ...      ...  ...   ...  ...   ...  \n",
      "158          0     0     0      0       0        0    0     0    0     0  \n",
      "159          0     0     0      0       0        0    0     0    0     0  \n",
      "160          0     0     0      0       0        0    0     0    0     0  \n",
      "161          0     0     0      0       0        0    0     0    0     0  \n",
      "162          0     0     0      0       0        0    0     0    0     0  \n",
      "\n",
      "[163 rows x 508 columns]\n"
     ]
    }
   ],
   "source": [
    "#CONVERT back to String\n",
    "preproDoc = [' '.join(sentence) for sentence in filtered_documents]\n",
    "vectorizer = CountVectorizer()\n",
    "dtm = vectorizer.fit_transform(preproDoc)\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(dtm_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 2)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Singular Value Decomposition\n",
    "\n",
    "svd = TruncatedSVD(algorithm='randomized', n_components=2)\n",
    "lsa = svd.fit_transform(dtm)\n",
    "lsa.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of term_vectors: (508, 2)\n"
     ]
    }
   ],
   "source": [
    "#Latent Semantic Index \n",
    "term_vector = svd.components_.T \n",
    "print(\"Shape of term_vectors:\", term_vector.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.26923697 -0.12351872]]\n"
     ]
    }
   ],
   "source": [
    "#Query Input\n",
    "query = \"All in the valley of Death. Leave my loneliness unbroken!â€”quit the bust above my door!\"\n",
    "tokenized_query = word_tokenize(query.lower())\n",
    "filtered_query = []\n",
    "allowed_words = []\n",
    "for word in sentence:\n",
    "    if word not in stop_words:\n",
    "        allowed_words.append(word)\n",
    "    filtered_query.append(allowed_words)\n",
    "\n",
    "postQuery = [' '.join(word) for word in filtered_query]\n",
    "query_dtm = vectorizer.transform([' '.join(postQuery)])\n",
    "query_lsa = svd.transform(query_dtm) \n",
    "print(query_lsa)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
